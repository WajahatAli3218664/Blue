"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_textbook=self.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[959],{2053(e,n,i){i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>u,frontMatter:()=>s,metadata:()=>r,toc:()=>a});var o=i(4848),t=i(8453);const s={},l="Module 4: Vision-Language-Action (VLA)",r={id:"module4/index",title:"Module 4: Vision-Language-Action (VLA)",description:"Welcome to Module 4! Learn to integrate natural language processing with robotic actions using Vision-Language-Action models.",source:"@site/docs/module4/index.md",sourceDirName:"module4",slug:"/module4/",permalink:"/docs/module4/",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Path Planning & Navigation",permalink:"/docs/module3/path-planning"},next:{title:"Voice-to-Action",permalink:"/docs/module4/voice-to-action"}},c={},a=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Topics in This Module",id:"topics-in-this-module",level:2},{value:"Overview",id:"overview",level:2}];function d(e){const n={a:"a",h1:"h1",h2:"h2",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"}),"\n",(0,o.jsx)(n.p,{children:"Welcome to Module 4! Learn to integrate natural language processing with robotic actions using Vision-Language-Action models."}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Implement voice-to-action systems"}),"\n",(0,o.jsx)(n.li,{children:"Use LLMs for cognitive planning"}),"\n",(0,o.jsx)(n.li,{children:"Build multimodal AI systems"}),"\n",(0,o.jsx)(n.li,{children:"Deploy VLA models on edge devices"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"topics-in-this-module",children:"Topics in This Module"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.a,{href:"./voice-to-action",children:"Voice-to-Action"})})," - Natural language robot control"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"VLA models enable robots to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Understand Natural Language"}),": Process voice commands and text instructions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Visual Understanding"}),": Interpret scenes and objects"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action Generation"}),": Convert understanding into robot actions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Cognitive Reasoning"}),": Plan complex multi-step tasks"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Ready to give your robot a voice? Let's start!"})]})}function u(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>l,x:()=>r});var o=i(6540);const t={},s=o.createContext(t);function l(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);