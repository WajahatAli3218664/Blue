"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_textbook=self.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[707],{5857(e,n,a){a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>s,metadata:()=>t,toc:()=>c});var r=a(4848),i=a(8453);const s={},o="Sensors & Environment Rendering",t={id:"module2/sensors-rendering",title:"Sensors & Environment Rendering",description:"Introduction",source:"@site/docs/module2/sensors-rendering.md",sourceDirName:"module2",slug:"/module2/sensors-rendering",permalink:"/docs/module2/sensors-rendering",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Physics Simulation",permalink:"/docs/module2/physics-simulation"},next:{title:"Module 3: The AI-Robot Brain (NVIDIA Isaac)",permalink:"/docs/module3/"}},l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Camera Sensor Simulation",id:"camera-sensor-simulation",level:2},{value:"Depth Camera (RealSense Simulation)",id:"depth-camera-realsense-simulation",level:2},{value:"LiDAR Sensor",id:"lidar-sensor",level:2},{value:"IMU Sensor",id:"imu-sensor",level:2},{value:"Sensor Data Processing",id:"sensor-data-processing",level:2},{value:"Environment Rendering",id:"environment-rendering",level:2},{value:"Hardware Integration",id:"hardware-integration",level:2},{value:"Next Steps",id:"next-steps",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"sensors--environment-rendering",children:"Sensors & Environment Rendering"}),"\n",(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsx)(n.p,{children:"Realistic sensor simulation is crucial for developing robust robotic systems. This topic covers camera, lidar, and IMU simulation."}),"\n",(0,r.jsx)(n.h2,{id:"camera-sensor-simulation",children:"Camera Sensor Simulation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'\x3c!-- RGB Camera --\x3e\n<gazebo reference="camera_link">\n  <sensor type="camera" name="camera">\n    <update_rate>30.0</update_rate>\n    <camera name="head">\n      <horizontal_fov>1.3962634</horizontal_fov>\n      <image>\n        <width>1920</width>\n        <height>1080</height>\n        <format>R8G8B8</format>\n      </image>\n      <clip>\n        <near>0.02</near>\n        <far>300</far>\n      </clip>\n      <noise>\n        <type>gaussian</type>\n        <mean>0.0</mean>\n        <stddev>0.007</stddev>\n      </noise>\n    </camera>\n    <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\n      <alwaysOn>true</alwaysOn>\n      <updateRate>0.0</updateRate>\n      <cameraName>robot/camera</cameraName>\n      <imageTopicName>image_raw</imageTopicName>\n      <cameraInfoTopicName>camera_info</cameraInfoTopicName>\n      <frameName>camera_link</frameName>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,r.jsx)(n.h2,{id:"depth-camera-realsense-simulation",children:"Depth Camera (RealSense Simulation)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Depth Camera --\x3e\n<gazebo reference="depth_camera_link">\n  <sensor type="depth" name="depth_camera">\n    <update_rate>20</update_rate>\n    <camera>\n      <horizontal_fov>1.047198</horizontal_fov>\n      <image>\n        <width>640</width>\n        <height>480</height>\n        <format>R8G8B8</format>\n      </image>\n      <clip>\n        <near>0.05</near>\n        <far>3</far>\n      </clip>\n    </camera>\n    <plugin name="depth_camera_controller" filename="libgazebo_ros_openni_kinect.so">\n      <baseline>0.2</baseline>\n      <alwaysOn>true</alwaysOn>\n      <updateRate>0.0</updateRate>\n      <cameraName>depth_camera_ir</cameraName>\n      <imageTopicName>/depth_camera/color/image_raw</imageTopicName>\n      <cameraInfoTopicName>/depth_camera/color/camera_info</cameraInfoTopicName>\n      <depthImageTopicName>/depth_camera/depth/image_raw</depthImageTopicName>\n      <depthImageCameraInfoTopicName>/depth_camera/depth/camera_info</depthImageCameraInfoTopicName>\n      <pointCloudTopicName>/depth_camera/depth/points</pointCloudTopicName>\n      <frameName>depth_camera_link</frameName>\n      <pointCloudCutoff>0.5</pointCloudCutoff>\n      <pointCloudCutoffMax>3.0</pointCloudCutoffMax>\n      <distortionK1>0</distortionK1>\n      <distortionK2>0</distortionK2>\n      <distortionK3>0</distortionK3>\n      <distortionT1>0</distortionT1>\n      <distortionT2>0</distortionT2>\n      <CxPrime>0</CxPrime>\n      <Cx>0</Cx>\n      <Cy>0</Cy>\n      <focalLength>0</focalLength>\n      <hackBaseline>0</hackBaseline>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,r.jsx)(n.h2,{id:"lidar-sensor",children:"LiDAR Sensor"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'\x3c!-- 2D LiDAR --\x3e\n<gazebo reference="lidar_link">\n  <sensor type="ray" name="hls_lfcd_lds">\n    <pose>0 0 0 0 0 0</pose>\n    <visualize>false</visualize>\n    <update_rate>5</update_rate>\n    <ray>\n      <scan>\n        <horizontal>\n          <samples>360</samples>\n          <resolution>1</resolution>\n          <min_angle>0.0</min_angle>\n          <max_angle>6.28319</max_angle>\n        </horizontal>\n      </scan>\n      <range>\n        <min>0.120</min>\n        <max>3.5</max>\n        <resolution>0.015</resolution>\n      </range>\n      <noise>\n        <type>gaussian</type>\n        <mean>0.0</mean>\n        <stddev>0.01</stddev>\n      </noise>\n    </ray>\n    <plugin name="gazebo_ros_lds_lfcd_controller" filename="libgazebo_ros_laser.so">\n      <topicName>scan</topicName>\n      <frameName>lidar_link</frameName>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,r.jsx)(n.h2,{id:"imu-sensor",children:"IMU Sensor"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'\x3c!-- IMU --\x3e\n<gazebo reference="imu_link">\n  <gravity>true</gravity>\n  <sensor name="imu_sensor" type="imu">\n    <always_on>true</always_on>\n    <update_rate>100</update_rate>\n    <visualize>true</visualize>\n    <topic>__default_topic__</topic>\n    <plugin filename="libgazebo_ros_imu_sensor.so" name="imu_plugin">\n      <topicName>imu</topicName>\n      <bodyName>imu_link</bodyName>\n      <updateRateHZ>10.0</updateRateHZ>\n      <gaussianNoise>0.0</gaussianNoise>\n      <xyzOffset>0 0 0</xyzOffset>\n      <rpyOffset>0 0 0</rpyOffset>\n      <frameName>imu_link</frameName>\n    </plugin>\n    <pose>0 0 0 0 0 0</pose>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,r.jsx)(n.h2,{id:"sensor-data-processing",children:"Sensor Data Processing"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan, Imu, PointCloud2\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass SensorProcessor(Node):\n    def __init__(self):\n        super().__init__(\'sensor_processor\')\n        self.bridge = CvBridge()\n        \n        # Subscribers\n        self.image_sub = self.create_subscription(Image, \'/robot/camera/image_raw\', self.image_callback, 10)\n        self.depth_sub = self.create_subscription(Image, \'/depth_camera/depth/image_raw\', self.depth_callback, 10)\n        self.lidar_sub = self.create_subscription(LaserScan, \'/scan\', self.lidar_callback, 10)\n        self.imu_sub = self.create_subscription(Imu, \'/imu\', self.imu_callback, 10)\n        \n    def image_callback(self, msg):\n        """Process RGB camera data"""\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\n            \n            # Object detection example\n            gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\n            circles = cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, 1, 20,\n                                     param1=50, param2=30, minRadius=0, maxRadius=0)\n            \n            if circles is not None:\n                circles = np.round(circles[0, :]).astype("int")\n                for (x, y, r) in circles:\n                    cv2.circle(cv_image, (x, y), r, (0, 255, 0), 4)\n                    \n            cv2.imshow("Camera Feed", cv_image)\n            cv2.waitKey(1)\n            \n        except Exception as e:\n            self.get_logger().error(f\'Image processing error: {e}\')\n    \n    def depth_callback(self, msg):\n        """Process depth camera data"""\n        try:\n            depth_image = self.bridge.imgmsg_to_cv2(msg, "32FC1")\n            \n            # Obstacle detection from depth\n            obstacles = np.where(depth_image < 1.0)  # Objects closer than 1m\n            if len(obstacles[0]) > 100:  # Significant obstacle\n                self.get_logger().warn(\'Obstacle detected!\')\n                \n        except Exception as e:\n            self.get_logger().error(f\'Depth processing error: {e}\')\n    \n    def lidar_callback(self, msg):\n        """Process LiDAR data"""\n        ranges = np.array(msg.ranges)\n        \n        # Find closest obstacle\n        valid_ranges = ranges[np.isfinite(ranges)]\n        if len(valid_ranges) > 0:\n            min_distance = np.min(valid_ranges)\n            if min_distance < 0.5:  # 50cm threshold\n                self.get_logger().warn(f\'Close obstacle at {min_distance:.2f}m\')\n    \n    def imu_callback(self, msg):\n        """Process IMU data"""\n        # Extract orientation and angular velocity\n        orientation = msg.orientation\n        angular_velocity = msg.angular_velocity\n        linear_acceleration = msg.linear_acceleration\n        \n        # Detect if robot is tilted\n        if abs(linear_acceleration.x) > 2.0 or abs(linear_acceleration.y) > 2.0:\n            self.get_logger().warn(\'Robot tilted!\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    sensor_processor = SensorProcessor()\n    rclpy.spin(sensor_processor)\n    sensor_processor.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"environment-rendering",children:"Environment Rendering"}),"\n",(0,r.jsx)(n.p,{children:"Create realistic environments:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Warehouse environment --\x3e\n<model name="warehouse">\n  <static>true</static>\n  <link name="link">\n    <collision name="collision">\n      <geometry>\n        <mesh>\n          <uri>model://warehouse/meshes/warehouse.dae</uri>\n        </mesh>\n      </geometry>\n    </collision>\n    <visual name="visual">\n      <geometry>\n        <mesh>\n          <uri>model://warehouse/meshes/warehouse.dae</uri>\n        </mesh>\n      </geometry>\n    </visual>\n  </link>\n</model>\n'})}),"\n",(0,r.jsx)(n.h2,{id:"hardware-integration",children:"Hardware Integration"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RealSense D435i"}),": Match simulation parameters with real sensor specs"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RTX GPUs"}),": Enable real-time ray tracing for realistic lighting"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Jetson Orin"}),": Optimize sensor processing for edge deployment"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsx)(n.p,{children:"In Module 3, we'll use these simulated sensors with NVIDIA Isaac for advanced perception and navigation."})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(m,{...e})}):m(e)}},8453(e,n,a){a.d(n,{R:()=>o,x:()=>t});var r=a(6540);const i={},s=r.createContext(i);function o(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);