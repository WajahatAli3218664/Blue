"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_textbook=self.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[399],{268(n,e,s){s.r(e),s.d(e,{assets:()=>r,contentTitle:()=>i,default:()=>g,frontMatter:()=>t,metadata:()=>l,toc:()=>c});var a=s(4848),o=s(8453);const t={},i="Obstacle Navigation",l={id:"module5/obstacle-navigation",title:"Obstacle Navigation",description:"Introduction",source:"@site/docs/module5/obstacle-navigation.md",sourceDirName:"module5",slug:"/module5/obstacle-navigation",permalink:"/docs/module5/obstacle-navigation",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Simulated Humanoid",permalink:"/docs/module5/simulated-humanoid"}},r={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Multi-Sensor Fusion",id:"multi-sensor-fusion",level:2},{value:"Advanced Navigation Strategies",id:"advanced-navigation-strategies",level:2},{value:"Hardware Integration",id:"hardware-integration",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.h1,{id:"obstacle-navigation",children:"Obstacle Navigation"}),"\n",(0,a.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(e.p,{children:"Advanced obstacle navigation for humanoid robots using multi-sensor fusion and AI-powered path planning."}),"\n",(0,a.jsx)(e.h2,{id:"multi-sensor-fusion",children:"Multi-Sensor Fusion"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Image, PointCloud2\nfrom geometry_msgs.msg import Twist, PoseStamped\nimport numpy as np\nimport cv2\nfrom cv_bridge import CvBridge\n\nclass ObstacleNavigationNode(Node):\n    def __init__(self):\n        super().__init__(\'obstacle_navigation\')\n        \n        # Subscribers\n        self.lidar_sub = self.create_subscription(LaserScan, \'/scan\', self.lidar_callback, 10)\n        self.camera_sub = self.create_subscription(Image, \'/humanoid/camera/image_raw\', self.camera_callback, 10)\n        self.depth_sub = self.create_subscription(PointCloud2, \'/depth_camera/points\', self.pointcloud_callback, 10)\n        self.goal_sub = self.create_subscription(PoseStamped, \'/goal_pose\', self.goal_callback, 10)\n        \n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.obstacle_pub = self.create_publisher(PointCloud2, \'/obstacles\', 10)\n        \n        # Navigation state\n        self.current_goal = None\n        self.obstacles = []\n        self.bridge = CvBridge()\n        \n        # Sensor fusion data\n        self.lidar_obstacles = []\n        self.vision_obstacles = []\n        self.depth_obstacles = []\n        \n        # Control timer\n        self.timer = self.create_timer(0.1, self.navigation_loop)\n        \n    def lidar_callback(self, msg):\n        """Process LiDAR data for obstacle detection"""\n        ranges = np.array(msg.ranges)\n        angles = np.linspace(msg.angle_min, msg.angle_max, len(ranges))\n        \n        # Filter valid ranges\n        valid_indices = np.isfinite(ranges) & (ranges > msg.range_min) & (ranges < msg.range_max)\n        valid_ranges = ranges[valid_indices]\n        valid_angles = angles[valid_indices]\n        \n        # Convert to Cartesian coordinates\n        obstacles = []\n        for r, theta in zip(valid_ranges, valid_angles):\n            if r < 2.0:  # Only consider obstacles within 2 meters\n                x = r * np.cos(theta)\n                y = r * np.sin(theta)\n                obstacles.append([x, y])\n        \n        self.lidar_obstacles = obstacles\n    \n    def camera_callback(self, msg):\n        """Process camera data for visual obstacle detection"""\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\n            \n            # Detect obstacles using computer vision\n            obstacles = self.detect_visual_obstacles(cv_image)\n            self.vision_obstacles = obstacles\n            \n        except Exception as e:\n            self.get_logger().error(f\'Camera processing error: {e}\')\n    \n    def detect_visual_obstacles(self, image):\n        """Detect obstacles using computer vision"""\n        # Convert to grayscale\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        \n        # Edge detection\n        edges = cv2.Canny(gray, 50, 150)\n        \n        # Find contours\n        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        \n        obstacles = []\n        for contour in contours:\n            # Filter by area\n            area = cv2.contourArea(contour)\n            if area > 1000:  # Minimum obstacle size\n                # Get bounding box\n                x, y, w, h = cv2.boundingRect(contour)\n                \n                # Estimate 3D position (simplified)\n                # This would normally use depth information\n                estimated_distance = 2000 / w  # Rough distance estimation\n                angle = (x + w/2 - image.shape[1]/2) * 0.001  # Rough angle\n                \n                obs_x = estimated_distance * np.cos(angle)\n                obs_y = estimated_distance * np.sin(angle)\n                obstacles.append([obs_x, obs_y])\n        \n        return obstacles\n    \n    def pointcloud_callback(self, msg):\n        """Process point cloud data"""\n        # Convert point cloud to obstacle list\n        # This would use pcl_ros or similar for processing\n        pass\n    \n    def goal_callback(self, msg):\n        """Set new navigation goal"""\n        self.current_goal = msg\n        self.get_logger().info(f\'New goal: ({msg.pose.position.x:.2f}, {msg.pose.position.y:.2f})\')\n    \n    def navigation_loop(self):\n        """Main navigation control loop"""\n        if not self.current_goal:\n            return\n        \n        # Fuse sensor data\n        all_obstacles = self.fuse_obstacle_data()\n        \n        # Plan path\n        velocity_cmd = self.plan_velocity(all_obstacles)\n        \n        # Publish velocity command\n        self.cmd_vel_pub.publish(velocity_cmd)\n    \n    def fuse_obstacle_data(self):\n        """Fuse obstacles from multiple sensors"""\n        all_obstacles = []\n        \n        # Add LiDAR obstacles (high confidence)\n        for obs in self.lidar_obstacles:\n            all_obstacles.append({\n                \'position\': obs,\n                \'confidence\': 0.9,\n                \'source\': \'lidar\'\n            })\n        \n        # Add vision obstacles (medium confidence)\n        for obs in self.vision_obstacles:\n            all_obstacles.append({\n                \'position\': obs,\n                \'confidence\': 0.6,\n                \'source\': \'vision\'\n            })\n        \n        # Remove duplicates and merge nearby obstacles\n        fused_obstacles = self.merge_nearby_obstacles(all_obstacles)\n        \n        return fused_obstacles\n    \n    def merge_nearby_obstacles(self, obstacles):\n        """Merge obstacles that are close to each other"""\n        merged = []\n        merge_distance = 0.3  # 30cm threshold\n        \n        for obs in obstacles:\n            merged_with_existing = False\n            \n            for existing in merged:\n                distance = np.linalg.norm(\n                    np.array(obs[\'position\']) - np.array(existing[\'position\'])\n                )\n                \n                if distance < merge_distance:\n                    # Merge obstacles (weighted average)\n                    total_conf = obs[\'confidence\'] + existing[\'confidence\']\n                    existing[\'position\'] = [\n                        (obs[\'position\'][0] * obs[\'confidence\'] + \n                         existing[\'position\'][0] * existing[\'confidence\']) / total_conf,\n                        (obs[\'position\'][1] * obs[\'confidence\'] + \n                         existing[\'position\'][1] * existing[\'confidence\']) / total_conf\n                    ]\n                    existing[\'confidence\'] = min(1.0, total_conf)\n                    merged_with_existing = True\n                    break\n            \n            if not merged_with_existing:\n                merged.append(obs)\n        \n        return merged\n    \n    def plan_velocity(self, obstacles):\n        """Plan velocity using dynamic window approach"""\n        goal_x = self.current_goal.pose.position.x\n        goal_y = self.current_goal.pose.position.y\n        \n        # Current robot state (simplified)\n        robot_x, robot_y, robot_theta = 0.0, 0.0, 0.0\n        \n        # Calculate goal direction\n        goal_distance = np.sqrt(goal_x**2 + goal_y**2)\n        goal_angle = np.arctan2(goal_y, goal_x)\n        \n        # Dynamic window parameters\n        max_linear_vel = 0.5\n        max_angular_vel = 1.0\n        \n        # Simple obstacle avoidance\n        cmd = Twist()\n        \n        if goal_distance < 0.1:\n            # Reached goal\n            cmd.linear.x = 0.0\n            cmd.angular.z = 0.0\n            self.current_goal = None\n            self.get_logger().info(\'Goal reached!\')\n        else:\n            # Check for obstacles in path\n            obstacle_in_path = False\n            min_obstacle_distance = float(\'inf\')\n            \n            for obs in obstacles:\n                obs_distance = np.linalg.norm(obs[\'position\'])\n                obs_angle = np.arctan2(obs[\'position\'][1], obs[\'position\'][0])\n                \n                # Check if obstacle is in our path\n                if (abs(obs_angle - goal_angle) < 0.5 and \n                    obs_distance < 1.0 and \n                    obs[\'confidence\'] > 0.5):\n                    obstacle_in_path = True\n                    min_obstacle_distance = min(min_obstacle_distance, obs_distance)\n            \n            if obstacle_in_path:\n                # Obstacle avoidance behavior\n                cmd.linear.x = max(0.1, min_obstacle_distance - 0.5)\n                \n                # Turn away from obstacle\n                if goal_angle > 0:\n                    cmd.angular.z = max_angular_vel * 0.5\n                else:\n                    cmd.angular.z = -max_angular_vel * 0.5\n            else:\n                # Move toward goal\n                cmd.linear.x = min(max_linear_vel, goal_distance * 0.5)\n                cmd.angular.z = goal_angle * 0.8\n        \n        return cmd\n\nclass AdvancedPathPlanner:\n    def __init__(self):\n        self.grid_resolution = 0.1\n        self.robot_radius = 0.3\n        \n    def plan_path(self, start, goal, obstacles):\n        """Plan path using A* with obstacle inflation"""\n        # Create occupancy grid\n        grid = self.create_occupancy_grid(obstacles)\n        \n        # Inflate obstacles by robot radius\n        inflated_grid = self.inflate_obstacles(grid)\n        \n        # Run A* algorithm\n        path = self.astar(start, goal, inflated_grid)\n        \n        return path\n    \n    def create_occupancy_grid(self, obstacles):\n        """Create occupancy grid from obstacle list"""\n        # Implementation for grid creation\n        pass\n    \n    def inflate_obstacles(self, grid):\n        """Inflate obstacles by robot radius"""\n        # Implementation for obstacle inflation\n        pass\n    \n    def astar(self, start, goal, grid):\n        """A* path planning algorithm"""\n        # Implementation of A* algorithm\n        pass\n\ndef main(args=None):\n    rclpy.init(args=args)\n    navigation_node = ObstacleNavigationNode()\n    rclpy.spin(navigation_node)\n    navigation_node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(e.h2,{id:"advanced-navigation-strategies",children:"Advanced Navigation Strategies"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"class HumanoidNavigationStrategies:\n    def __init__(self):\n        self.strategies = {\n            'narrow_passage': self.navigate_narrow_passage,\n            'stairs': self.navigate_stairs,\n            'dynamic_obstacles': self.navigate_dynamic_obstacles,\n            'crowded_space': self.navigate_crowded_space\n        }\n    \n    def navigate_narrow_passage(self, passage_width):\n        \"\"\"Navigate through narrow passages\"\"\"\n        if passage_width < 0.8:  # Tight passage\n            return {\n                'linear_velocity': 0.2,  # Slow and careful\n                'angular_velocity': 0.0,\n                'body_posture': 'narrow'  # Adjust arm positions\n            }\n    \n    def navigate_stairs(self, stair_detection):\n        \"\"\"Navigate stairs (up or down)\"\"\"\n        return {\n            'gait_pattern': 'stair_climbing',\n            'step_height': stair_detection['step_height'],\n            'handrail_detection': True\n        }\n    \n    def navigate_dynamic_obstacles(self, moving_obstacles):\n        \"\"\"Navigate around moving obstacles (people, other robots)\"\"\"\n        predictions = self.predict_obstacle_motion(moving_obstacles)\n        return self.plan_with_predictions(predictions)\n    \n    def predict_obstacle_motion(self, obstacles):\n        \"\"\"Predict future positions of moving obstacles\"\"\"\n        predictions = []\n        for obs in obstacles:\n            # Simple linear prediction\n            future_pos = [\n                obs['position'][0] + obs['velocity'][0] * 2.0,  # 2 seconds ahead\n                obs['position'][1] + obs['velocity'][1] * 2.0\n            ]\n            predictions.append(future_pos)\n        return predictions\n"})}),"\n",(0,a.jsx)(e.h2,{id:"hardware-integration",children:"Hardware Integration"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Multi-Sensor Setup"}),": LiDAR, cameras, depth sensors, IMU"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Real-time Processing"}),": Optimize for low-latency navigation"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Safety Systems"}),": Emergency stop and collision avoidance"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Edge Deployment"}),": Run navigation stack on Jetson Orin Nano"]}),"\n"]})]})}function g(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}},8453(n,e,s){s.d(e,{R:()=>i,x:()=>l});var a=s(6540);const o={},t=a.createContext(o);function i(n){const e=a.useContext(t);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:i(n.components),a.createElement(t.Provider,{value:e},n.children)}}}]);