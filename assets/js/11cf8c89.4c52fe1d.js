"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_textbook=self.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[960],{2708(n,e,o){o.r(e),o.d(e,{assets:()=>c,contentTitle:()=>s,default:()=>m,frontMatter:()=>r,metadata:()=>a,toc:()=>l});var t=o(4848),i=o(8453);const r={},s="Voice-to-Action",a={id:"module4/voice-to-action",title:"Voice-to-Action",description:"Introduction",source:"@site/docs/module4/voice-to-action.md",sourceDirName:"module4",slug:"/module4/voice-to-action",permalink:"/docs/module4/voice-to-action",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Module 4: Vision-Language-Action (VLA)",permalink:"/docs/module4/"},next:{title:"Module 5: Capstone Autonomous Humanoid Project",permalink:"/docs/module5/"}},c={},l=[{value:"Introduction",id:"introduction",level:2},{value:"Speech Recognition Setup",id:"speech-recognition-setup",level:2},{value:"Advanced Natural Language Processing",id:"advanced-natural-language-processing",level:2},{value:"Hardware Integration",id:"hardware-integration",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.h1,{id:"voice-to-action",children:"Voice-to-Action"}),"\n",(0,t.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsx)(e.p,{children:"Voice-to-Action systems enable robots to understand and execute natural language commands through speech recognition and action mapping."}),"\n",(0,t.jsx)(e.h2,{id:"speech-recognition-setup",children:"Speech Recognition Setup"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nimport speech_recognition as sr\nimport pyttsx3\nfrom geometry_msgs.msg import Twist\nimport threading\n\nclass VoiceControlNode(Node):\n    def __init__(self):\n        super().__init__(\'voice_control_node\')\n        \n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        \n        # Speech recognition\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n        \n        # Text-to-speech\n        self.tts_engine = pyttsx3.init()\n        self.tts_engine.setProperty(\'rate\', 150)\n        \n        # Voice commands mapping\n        self.commands = {\n            \'move forward\': self.move_forward,\n            \'move backward\': self.move_backward,\n            \'turn left\': self.turn_left,\n            \'turn right\': self.turn_right,\n            \'stop\': self.stop_robot,\n            \'go to kitchen\': lambda: self.navigate_to_location(\'kitchen\'),\n            \'go to bedroom\': lambda: self.navigate_to_location(\'bedroom\'),\n        }\n        \n        # Start listening thread\n        self.listening_thread = threading.Thread(target=self.listen_continuously)\n        self.listening_thread.daemon = True\n        self.listening_thread.start()\n        \n        self.get_logger().info(\'Voice control node started. Say commands!\')\n    \n    def listen_continuously(self):\n        """Continuously listen for voice commands"""\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source)\n        \n        while rclpy.ok():\n            try:\n                with self.microphone as source:\n                    # Listen for audio with timeout\n                    audio = self.recognizer.listen(source, timeout=1, phrase_time_limit=5)\n                \n                # Recognize speech\n                command = self.recognizer.recognize_google(audio).lower()\n                self.get_logger().info(f\'Heard: "{command}"\')\n                \n                # Process command\n                self.process_command(command)\n                \n            except sr.WaitTimeoutError:\n                pass  # No speech detected\n            except sr.UnknownValueError:\n                self.speak("Sorry, I didn\'t understand that.")\n            except sr.RequestError as e:\n                self.get_logger().error(f\'Speech recognition error: {e}\')\n    \n    def process_command(self, command):\n        """Process recognized voice command"""\n        # Find matching command\n        for key, action in self.commands.items():\n            if key in command:\n                self.speak(f"Executing: {key}")\n                action()\n                return\n        \n        # If no direct match, try LLM interpretation\n        self.interpret_with_llm(command)\n    \n    def interpret_with_llm(self, command):\n        """Use LLM to interpret complex commands"""\n        # This would integrate with your LLM API\n        prompt = f"""\n        Convert this natural language command to robot action:\n        Command: "{command}"\n        \n        Available actions:\n        - move_forward()\n        - move_backward()\n        - turn_left()\n        - turn_right()\n        - stop_robot()\n        - navigate_to_location(location)\n        \n        Return the function call:\n        """\n        \n        # Call LLM API here\n        # For now, simple fallback\n        self.speak("I\'m not sure how to do that yet.")\n    \n    def speak(self, text):\n        """Convert text to speech"""\n        self.tts_engine.say(text)\n        self.tts_engine.runAndWait()\n    \n    def move_forward(self):\n        """Move robot forward"""\n        cmd = Twist()\n        cmd.linear.x = 0.5\n        self.cmd_vel_pub.publish(cmd)\n        \n    def move_backward(self):\n        """Move robot backward"""\n        cmd = Twist()\n        cmd.linear.x = -0.5\n        self.cmd_vel_pub.publish(cmd)\n        \n    def turn_left(self):\n        """Turn robot left"""\n        cmd = Twist()\n        cmd.angular.z = 0.5\n        self.cmd_vel_pub.publish(cmd)\n        \n    def turn_right(self):\n        """Turn robot right"""\n        cmd = Twist()\n        cmd.angular.z = -0.5\n        self.cmd_vel_pub.publish(cmd)\n        \n    def stop_robot(self):\n        """Stop robot movement"""\n        cmd = Twist()\n        self.cmd_vel_pub.publish(cmd)\n        \n    def navigate_to_location(self, location):\n        """Navigate to specific location"""\n        self.get_logger().info(f\'Navigating to {location}\')\n        # Integrate with navigation stack\n\ndef main(args=None):\n    rclpy.init(args=args)\n    voice_control = VoiceControlNode()\n    rclpy.spin(voice_control)\n    voice_control.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(e.h2,{id:"advanced-natural-language-processing",children:"Advanced Natural Language Processing"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"import openai\nfrom transformers import pipeline\n\nclass AdvancedNLP:\n    def __init__(self):\n        # Initialize models\n        self.intent_classifier = pipeline(\"text-classification\", \n                                         model=\"microsoft/DialoGPT-medium\")\n        self.ner_model = pipeline(\"ner\", \n                                model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n    \n    def extract_intent_and_entities(self, command):\n        \"\"\"Extract intent and entities from command\"\"\"\n        # Intent classification\n        intent = self.classify_intent(command)\n        \n        # Named entity recognition\n        entities = self.ner_model(command)\n        \n        return intent, entities\n    \n    def classify_intent(self, command):\n        \"\"\"Classify the intent of the command\"\"\"\n        intents = {\n            'navigation': ['go', 'move', 'navigate', 'walk'],\n            'manipulation': ['pick', 'grab', 'place', 'put'],\n            'observation': ['look', 'see', 'find', 'search'],\n            'interaction': ['say', 'tell', 'ask', 'speak']\n        }\n        \n        command_lower = command.lower()\n        for intent, keywords in intents.items():\n            if any(keyword in command_lower for keyword in keywords):\n                return intent\n        \n        return 'unknown'\n"})}),"\n",(0,t.jsx)(e.h2,{id:"hardware-integration",children:"Hardware Integration"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Edge Deployment"}),": Optimize speech recognition for Jetson Orin Nano"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Cloud Processing"}),": Use AWS Transcribe for advanced speech recognition"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Real-time Performance"}),": Balance accuracy with latency requirements"]}),"\n"]})]})}function m(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453(n,e,o){o.d(e,{R:()=>s,x:()=>a});var t=o(6540);const i={},r=t.createContext(i);function s(n){const e=t.useContext(r);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:s(n.components),t.createElement(r.Provider,{value:e},n.children)}}}]);